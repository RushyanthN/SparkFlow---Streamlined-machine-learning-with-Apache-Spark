# -*- coding: utf-8 -*-
"""Spark_ML_assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZjFsRBBzdPqLStOCJA3HFfHL-wy-6mZP
"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz
!tar xf spark-3.2.0-bin-hadoop3.2.tgz
!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "spark-3.2.0-bin-hadoop3.2"

import findspark
findspark.init()

from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()
sc = spark.sparkContext
sc

file_path = "/content/nyc_taxi.csv"
df = spark.read.csv(file_path,header = True, inferSchema = True)
df.show()

df.printSchema()

from pyspark.sql.functions import col
df = df.filter((col("fare") >= 0) & (col("tip") >= 0))

fare_threshold = df.approxQuantile("fare", [0.99], 0.01)[0]
df = df.filter(col("fare") <= fare_threshold)

# Calculate Q1 and Q3 for 'distance', 'tip', and 'fare'
Q1_distance, Q3_distance = df.approxQuantile("distance", [0.25, 0.75], 0.01)
Q1_tip, Q3_tip = df.approxQuantile("tip", [0.25, 0.75], 0.01)
Q1_fare, Q3_fare = df.approxQuantile("fare", [0.25, 0.75], 0.01)

# Calculate IQR
IQR_distance = Q3_distance - Q1_distance
IQR_tip = Q3_tip - Q1_tip
IQR_fare = Q3_fare - Q1_fare

# Define the IQR filter range
df = df.filter(
    (col("distance") >= Q1_distance - 1.5 * IQR_distance) & (col("distance") <= Q3_distance + 1.5 * IQR_distance) &
    (col("tip") >= Q1_tip - 1.5 * IQR_tip) & (col("tip") <= Q3_tip + 1.5 * IQR_tip) &
    (col("fare") >= Q1_fare - 1.5 * IQR_fare) & (col("fare") <= Q3_fare + 1.5 * IQR_fare)
)

df.show()

df.describe().show()

# Set legacy time parser policy to handle older formats
spark.conf.set("spark.sql.legacy.timeParserPolicy", "LEGACY")

# Continue with your code
df = df.withColumn('pickup_datetime',
    concat(col('pickup_date'), lit(' '), col('pickup_time')))
df = df.withColumn('dropoff_datetime',
    concat(col('dropoff_date'), lit(' '), col('dropoff_time')))

# Convert to unix timestamp and calculate difference in minutes
df = df.withColumn('duration',
    (unix_timestamp(col('dropoff_datetime'), 'M/d/yyyy HH:mm') -
     unix_timestamp(col('pickup_datetime'), 'M/d/yyyy HH:mm')) / 60)

df.describe().show()

from pyspark.sql.functions import col, when
time_windows = [
    (4, 6, "Early Morning"),
    (6, 8, "Morning"),
    (8, 12, "Mid Morning"),
    (12, 14, "Noon"),
    (14, 17, "Afternoon"),
    (17, 20, "Evening"),
    (20, 24, "Night"),
    (0, 4, "Late Night")
]

# Create a new column 'time_of_day' based on the pickup time
df = df.withColumn(
    "time_of_day",
    when(col("pickup_time").substr(1, 2).cast("int").between(4, 5), "Early Morning")
    .when(col("pickup_time").substr(1, 2).cast("int").between(6, 7), "Morning")
    .when(col("pickup_time").substr(1, 2).cast("int").between(8, 11), "Mid Morning")
    .when(col("pickup_time").substr(1, 2).cast("int").between(12, 13), "Noon")
    .when(col("pickup_time").substr(1, 2).cast("int").between(14, 16), "Afternoon")
    .when(col("pickup_time").substr(1, 2).cast("int").between(17, 19), "Evening")
    .when(col("pickup_time").substr(1, 2).cast("int").between(20, 23), "Night")
    .otherwise("Late Night")
)

df.show()

from pyspark.ml.regression import LinearRegression
from pyspark.ml.feature import VectorAssembler

assembler = VectorAssembler(inputCols=["distance"], outputCol="features")
df_assembled = assembler.transform(df)

train_data, test_data = df_assembled.randomSplit([0.8, 0.2], seed=42)

lr = LinearRegression(featuresCol="features", labelCol="fare")
model_m1 = lr.fit(train_data)

predictions = model_m1.transform(test_data)

from pyspark.ml.evaluation import RegressionEvaluator
evaluator = RegressionEvaluator(labelCol="fare", predictionCol="prediction", metricName="r2")
r2 = evaluator.evaluate(predictions)
print(f"R-squared (M1): {r2}")

predictions.select("fare", "prediction", "distance").show()

from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.sql.functions import lit, concat, unix_timestamp, col

assembler_m2 = VectorAssembler(inputCols=["distance", "duration"], outputCol="features")
df_assembled_m2 = assembler_m2.transform(df)

train_data_m2, test_data_m2 = df_assembled_m2.randomSplit([0.8, 0.2], seed=42)
lr_m2 = LinearRegression(featuresCol="features", labelCol="fare")
model_m2 = lr_m2.fit(train_data_m2)


temp_df = spark.createDataFrame([(20.0,)], ["distance"])
temp_df = assembler.transform(temp_df)  # Use the 'assembler' from M1, not a new one
prediction_m1_20miles = model_m1.transform(temp_df)
print(f"Fare of a 20 mile long trip using M1: {prediction_m1_20miles.select('prediction').first()[0]}")


temp_df_m2 = spark.createDataFrame([(14.0, 75.0)], ["distance", "duration"])
temp_df_m2 = assembler_m2.transform(temp_df_m2)
prediction_m2_14miles_75min = model_m2.transform(temp_df_m2)
print(f"Fare of a 14 mile trip that took 75 minutes using M2: {prediction_m2_14miles_75min.select('prediction').first()[0]}")



trip1_df = spark.createDataFrame([(10.0, 40.0)], ["distance", "duration"])
trip1_df = assembler_m2.transform(trip1_df)
trip1 = model_m2.transform(trip1_df)

trip2_df = spark.createDataFrame([(13.0, 25.0)], ["distance", "duration"])
trip2_df = assembler_m2.transform(trip2_df)
trip2 = model_m2.transform(trip2_df)

trip1_fare = trip1.select("prediction").first()[0]
trip2_fare = trip2.select("prediction").first()[0]

print(f"10 mile trip taking 40 min fare: {trip1_fare}")
print(f"13 mile trip taking 25 min fare: {trip2_fare}")

if trip1_fare > trip2_fare:
    print("The 10 mile trip taking 40 min has a higher fare.")
elif trip2_fare > trip1_fare:
    print("The 13 mile trip taking 25 min has a higher fare.")
else:
    print("Both trips have the same fare.")

average_tip = df.agg({"tip": "avg"}).collect()[0][0]
print(f"The average tip amount is: {average_tip}")

from pyspark.sql.functions import hour, count, when, concat, lit

df = df.withColumn("pickup_hour", hour(col("pickup_time")))

df = df.withColumn("pickup_period",
                   when(col("pickup_hour") < 12, concat(col("pickup_hour"), lit(" AM")))
                   .when(col("pickup_hour") == 12, lit("12 PM"))
                   .when(col("pickup_hour") > 12, concat((col("pickup_hour") - 12), lit(" PM"))))

df = df.withColumn("next_hour_period",
                   when(col("pickup_hour") < 11, concat((col("pickup_hour") + 1), lit(" AM")))
                   .when(col("pickup_hour") == 11, lit("12 PM"))
                   .when(col("pickup_hour") == 23, lit("12 AM"))
                   .when(col("pickup_hour") == 12, lit("1 PM"))
                   .when(col("pickup_hour") > 12, concat((col("pickup_hour") - 11), lit(" PM"))))

hourly_trips = df.groupBy("pickup_hour", "pickup_period", "next_hour_period").agg(count("*").alias("trip_count"))
max_trips_interval = hourly_trips.orderBy(col("trip_count").desc()).first()
print(f"The time interval with the most trips is from {max_trips_interval['pickup_period']} to {max_trips_interval['next_hour_period']}, with {max_trips_interval['trip_count']} trips.")
hourly_trips.orderBy("pickup_hour").select("pickup_period", "next_hour_period", "trip_count").show(24, truncate=False)

import matplotlib.pyplot as plt
import time


def time_function(func, data, iterations=1):
    total_time = 0
    for _ in range(iterations):
        start_time = time.time()
        func(data)
        end_time = time.time()
        total_time += (end_time - start_time)
    return total_time / iterations

fractions = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
times_m1 = []
times_m2 = []

for fraction in fractions:
    # Sample a fraction of the data
    sampled_df = df.sample(fraction=fraction, seed=42)

    # Time M1
    def run_m1(data):
        assembler = VectorAssembler(inputCols=["distance"], outputCol="features")
        df_assembled = assembler.transform(data)
        train_data, test_data = df_assembled.randomSplit([0.8, 0.2], seed=42)
        lr = LinearRegression(featuresCol="features", labelCol="fare")
        model = lr.fit(train_data)
        predictions = model.transform(test_data)

    times_m1.append(time_function(run_m1, sampled_df))

    # Time M2
    def run_m2(data):
        assembler = VectorAssembler(inputCols=["distance", "duration"], outputCol="features")
        df_assembled = assembler.transform(data)
        train_data, test_data = df_assembled.randomSplit([0.8, 0.2], seed=42)
        lr = LinearRegression(featuresCol="features", labelCol="fare")
        model = lr.fit(train_data)
        predictions = model.transform(test_data)

    times_m2.append(time_function(run_m2, sampled_df))

plt.figure(figsize=(10, 6))
plt.plot(fractions, times_m1, label="M1 (distance only)")
plt.plot(fractions, times_m2, label="M2 (distance and duration)")
plt.xlabel("Fraction of Data")
plt.ylabel("Time (seconds)")
plt.title("Spark Performance Comparison")
plt.legend()
plt.grid(True)
plt.savefig("spark_performance.png") #Save the plot
plt.show()

